{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PNL Preprocessing\n",
    "\n",
    "## !! Requirements\n",
    "\n",
    "You need at least 16GB of memory to run this notebook. It's recommended to have 32GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting user data in train and test\n",
    "\n",
    "We only divided the user dataset with SKLearn. The Tweets we filtered, based on the group that each tweet was in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting user data\n",
    "df_tradicionalbot_us = pd.read_csv('datasets_full/traditional_spambots_1/users.csv')\n",
    "df_genuine_us = pd.read_csv('datasets_full/genuine_accounts.csv/users.csv')\n",
    "df_genuine_us = df_genuine_us.drop(['test_set_1','test_set_2'], axis=1)\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(df_tradicionalbot_us, np.ones(df_tradicionalbot_us['id'].unique().shape[0]), random_state=42)\n",
    "X_train_g, X_test_g, y_train_g, y_test_g = train_test_split(df_genuine_us, np.zeros(df_genuine_us['id'].unique().shape[0]), random_state=42)\n",
    "\n",
    "X_train = pd.concat([X_train_b, X_train_g])\n",
    "y_train = np.hstack([y_train_b, y_train_g])\n",
    "\n",
    "X_test = pd.concat([X_test_b, X_test_g])\n",
    "y_test = np.hstack([y_test_b, y_test_g])\n",
    "\n",
    "y_test = pd.DataFrame(y_test)\n",
    "y_test.index = X_test['id'].values\n",
    "y_test.to_csv(\"y_test.csv\")\n",
    "X_test.to_csv(\"x_test.csv\")\n",
    "\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_train.index = X_train['id'].values\n",
    "y_train.to_csv(\"y_train.csv\")\n",
    "X_train.to_csv(\"x_train.csv\")\n",
    "\n",
    "df_tradicionalbot_us, df_genuine_us, X_train_b, X_test_b, y_train_b, y_test_b, X_train_g, X_test_g, y_train_g, y_test_g, X_train, y_train, X_test, y_test = None, None, None,None,None,None,None,None,None,None,None,None,None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.read_csv(\"x_test.csv\")\n",
    "x_train = pd.read_csv(\"x_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #spliting tweet data\n",
    "x_test.set_index(\"id\",inplace=True)\n",
    "x_train.set_index(\"id\",inplace=True)\n",
    "\n",
    "aaa = [\"id\",\"text\",\"source\",\"user_id\",\"in_reply_to_user_id\",\"num_hashtags\",\"num_urls\",\"num_mentions\",\"timestamp\"]\n",
    "cols = [\"col \" + str(i) for i in range(28)]\n",
    "for x,y in zip(aaa,[0,1,2,3,6,19,20,21,22]):\n",
    "    cols[y] = x\n",
    "\n",
    "gen_tweets = pd.read_csv(\"datasets_full/genuine_accounts.csv/tweets.csv\",error_bad_lines=False,quotechar='\"',encoding='UTF-8', names=cols)\n",
    "gen_tweets = gen_tweets[aaa]\n",
    "\n",
    "def func(x):\n",
    "    try:\n",
    "        a = int(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "gen_tweets = gen_tweets[gen_tweets['user_id'].apply(lambda x: func(x))]\n",
    "gen_tweets['user_id'] = gen_tweets['user_id'].apply(lambda x: int(x))\n",
    "\n",
    "cols = [\"id\",\"text\",\"source\",\"user_id\",\"truncated\",\"in_reply_to_status_id\",\"in_reply_to_user_id\",\"in_reply_to_screen_name\",\"retweeted_status_id\",\"geo\",\"place\",\"contributors\",\"retweet_count\",\"reply_count\",\"favorite_count\",\"favorited\",\"retweeted\",\"possibly_sensitive\",\"num_hashtags\",\"num_urls\",\"num_mentions\",\"created_at\",\"timestamp\",\"crawled_at\",\"updated\"]\n",
    "\n",
    "spam_tweets = pd.read_excel(\"datasets_full/traditional_spambots_1/tweets.xlsx\") #I opened the tweets.csv at Excel, and then saved as xlsx. Was what permitted me to open this with Pandas.\n",
    "spam_tweets = spam_tweets[aaa]\n",
    "\n",
    "spam_tweets = spam_tweets[spam_tweets['user_id'].apply(lambda x: func(x))]\n",
    "spam_tweets['user_id'] = spam_tweets['user_id'].apply(lambda x: int(x))\n",
    "\n",
    "merged = spam_tweets.append(gen_tweets) \n",
    "\n",
    "train_index = set(x_train.index)\n",
    "tweet_train = merged[merged['user_id'].apply(lambda x: x in train_index)]\n",
    "\n",
    "test_index = set(x_test.index)\n",
    "tweet_test = merged[merged['user_id'].apply(lambda x: x in test_index)]\n",
    "\n",
    "tweet_test.to_csv(\"x_test_small.csv\")\n",
    "tweet_train.to_csv(\"x_train_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, aaa, cols, gen_tweets, spam_tweets, merged, train_index, tweet_train, test_index, tweet_test = None, None,None,None,None,None,None,None,None,None,None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Tweets\n",
    "\n",
    "The section bellow is very hard to run. There are sections that consume 14GB of memory. Mind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"x_train_small.csv\")\n",
    "df['is_reply'] = df['in_reply_to_user_id'].apply(lambda x: 0 if x == 0 else 1)\n",
    "df.drop([\"source\",\"timestamp\",\"Unnamed: 0\",\"id\",'in_reply_to_user_id'],axis=1,inplace=True)\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "tweets = df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the libraries NLTK and Gensim to generate the most important tokens from the tweets. It's called lemmatize in the library. Then, we create a gensim dictionary, with only 200 words. It's very time consuming this section of the code, so we didn't fell the need of selecting this hyper-parameter, because the results were satisfing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_docs = tweets.map(preprocess)\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_above=0.5, keep_n=200)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(tweets)\n",
    "X['ID'] = X.index\n",
    "\n",
    "class aaa: ##just a dummy class just to create the ID Collumn. \n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "\n",
    "    def func(self,x):\n",
    "        ans = self.count\n",
    "        self.count += 1\n",
    "        return ans\n",
    "\n",
    "a = aaa()\n",
    "X['ID'] =X['ID'].apply(lambda x: a.func(x))\n",
    "\n",
    "def calcula(word_index: int, tweet_index: int) -> int:\n",
    "    try:\n",
    "        list_tuplas = bow_corpus[tweet_index]\n",
    "        for i, count in list_tuplas:\n",
    "            if i == word_index:\n",
    "                return count\n",
    "        return 0\n",
    "    except: #used for debugging\n",
    "        print(word_index)\n",
    "        print(tweet_index)\n",
    "        raise Exception\n",
    "\n",
    "for word_index in range(200):\n",
    "    x = pd.DataFrame(X['ID'].apply(lambda tweet_index: calcula(word_index,tweet_index))) #for each tweet and for each word we match the previusly calculated data in the bow_corpus data structure\n",
    "    x.to_pickle(str(\"words/\" + dictionary[word_index]) + \".pkl\") #here we save it to a pickle to help our memory \n",
    "    print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for word_index in range(200):\n",
    "    df[dictionary[word_index]] = pd.read_pickle(\"words/\" + dictionary[word_index] + \".pkl\") #just here we add all to the dataset\n",
    "    print(word_index)\n",
    "\n",
    "df.to_csv(\"df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same thing, for the test data\n",
    "df_test = pd.read_csv(\"x_test_small.csv\")  \n",
    "df_test['is_reply'] = df_test['in_reply_to_user_id'].apply(lambda x: 0 if x == 0 else 1)\n",
    "df_test.drop([\"source\",\"timestamp\",\"Unnamed: 0\",\"id\",'in_reply_to_user_id'],axis=1,inplace=True)\n",
    "df_test.dropna(subset=['text'], inplace=True)\n",
    "tweets = df_test['text']\n",
    "\n",
    "processed_docs = tweets.map(preprocess)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "X = pd.DataFrame(tweets)\n",
    "a = aaa()\n",
    "X['ID'] =X['text'].apply(lambda x: a.func(x))\n",
    "\n",
    "for word_index in range(200):\n",
    "    x = pd.DataFrame(X['ID'].apply(lambda tweet_index: calcula(word_index,tweet_index)))\n",
    "    x.to_pickle(str(\"words_test/\" + dictionary[word_index]) + \".pkl\")\n",
    "    print(word_index)\n",
    "\n",
    "for word_index in range(200):\n",
    "    df_test[dictionary[word_index]] = pd.read_pickle(\"words/\" + dictionary[word_index] + \".pkl\")\n",
    "    print(word_index)\n",
    "\n",
    "df_test.to_csv(\"df_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the section bellow, we group the tweets by user using the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df.csv\")\n",
    "x_train = pd.read_csv(\"x_train_small.csv\")\n",
    "df['text'] = x_train['text']\n",
    "x_train = None\n",
    "gb = df.groupby(\"user_id\").mean()\n",
    "y_train = pd.read_csv(\"y_train.csv\")\n",
    "y_train.columns = [\"user_id\",\"ans\"]\n",
    "gb = gb.merge(y_train,on=\"user_id\",how=\"left\")\n",
    "gb.to_csv(\"ready_to_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_test.csv\")\n",
    "x_train = pd.read_csv(\"x_test_small.csv\")\n",
    "df['text'] = x_train['text']\n",
    "x_train = None\n",
    "gb = df.groupby(\"user_id\").mean()\n",
    "y_train = pd.read_csv(\"y_test.csv\")\n",
    "y_train.columns = [\"user_id\",\"ans\"]\n",
    "gb = gb.merge(y_train,on=\"user_id\",how=\"left\")\n",
    "gb.to_csv(\"ready_to_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this section, the tweets are ready to model."
   ]
  }
 ]
}
